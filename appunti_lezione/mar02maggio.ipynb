{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metriche Errore\n",
    "\n",
    "- Nella regressione lineare o multipla, una volta generato il modello predittivo, possiamo calcolarne l'errore rispetto ai punti tenuti per il test.\n",
    "- Errore è la differenza tra il target test (che non ha influito nella creazione del modello), e la predizione del modello negli stessi punti del test.\n",
    "\n",
    ">tipicamente nella regressione lineare/multipla i punti più estremi saranno affetti dall'errore maggiore in quanto la retta è molto rigida come \n",
    "\n",
    "## Errore nella regressione\n",
    "\n",
    "\n",
    "Le metriche dell'errore che ci possono aiutare a capire la bontà del modello sono:\n",
    "(si calcolano sulle variabile del test sono nel modulo metrics di scikitlearn)\n",
    "\n",
    "### Mean Absolute Error\n",
    "\n",
    "media degli errori valutati come differenze rispetto al valore predetto. Si rischia che gli errori si compensino.\n",
    "\n",
    "### mean squared error\n",
    "\n",
    "per evitare che gli errori si cancellino valuto il loro quadrato \n",
    "\n",
    "### Root Mean Square Error\n",
    "\n",
    "dalla mean square error faccio la radice quadrata e ritorno alla stessa dimensione della variabile di partenza.\n",
    "\n",
    "### Coefficiente di Determinazione\n",
    "**R-quadro-Score** numero compreso tra 0-1 che definisce quanto bene il modello si adatta ai dati.\n",
    "\n",
    "## Step per arrivare al modello\n",
    "\n",
    "1. definire qual'è il target (`y`) e quale parte dei dati è la feature (`X`)\n",
    "   1. suddividere sia target che feature in 2 insiemi Train e Test \n",
    "2. Capire quale può essere il modello migliore da generare per rappresentare il problema (LinearRegression, MultipleRegression, LogisticRegression, ecc...)\n",
    "3. Creare il Modello \n",
    "4. Calcolare la predizione usando la parte di test delle feature per ottenere una previsone della target generata dal modello\n",
    "5. Calcolare gli 'errori' del modello sulla base delle predizioni generate dal test rispetto alle target tenute da parte per il test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlem'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m LinearRegression, LogisticRegression\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmlem\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjoblib\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlem'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlem\n",
    "import joblib\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvare i modelli\n",
    "Generare un file binario che contiene il modello.\n",
    "Per generare questo file si usa Joblib.\n",
    "Usando il metodo dump `joblib.dump(mio_modello, prediction.pkl)` l'estensione non è importante ma tradizionalmente si usa pkl (pickle).\n",
    "\n",
    "Per usare il modello si usa il metodo `joblib.load(nome_modello.pkl)`\n",
    "\n",
    "Il training serve a generare il modello, trovato il modello lo salvo in binario e lo importo e uso quando serve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLEM\n",
    "\n",
    "Nuova libreria che funziona come joblib ma permette, tramite metatag, di far capire agli sviluppatori come utilizzare il modello cosa che con joblib non era possibile.\n",
    "I metodi da usare sono:\n",
    "- per la generazione `mlem.api.save(nome_modello, nome_modello_, sample_data= X_train)` dove nome_modello è il file binario, nome_modello_ è il file di metatag, sample_data= è l'array preso per generare il modello.\n",
    "- `mlel.api.load()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>131876.90</td>\n",
       "      <td>99814.71</td>\n",
       "      <td>362861.36</td>\n",
       "      <td>156991.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>134615.46</td>\n",
       "      <td>147198.87</td>\n",
       "      <td>127716.82</td>\n",
       "      <td>156122.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>130298.13</td>\n",
       "      <td>145530.06</td>\n",
       "      <td>323876.68</td>\n",
       "      <td>155752.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>120542.52</td>\n",
       "      <td>148718.95</td>\n",
       "      <td>311613.29</td>\n",
       "      <td>152211.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123334.88</td>\n",
       "      <td>108679.17</td>\n",
       "      <td>304981.62</td>\n",
       "      <td>149759.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>101913.08</td>\n",
       "      <td>110594.11</td>\n",
       "      <td>229160.95</td>\n",
       "      <td>146121.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100671.96</td>\n",
       "      <td>91790.61</td>\n",
       "      <td>249744.55</td>\n",
       "      <td>144259.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>93863.75</td>\n",
       "      <td>127320.38</td>\n",
       "      <td>249839.44</td>\n",
       "      <td>141585.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>91992.39</td>\n",
       "      <td>135495.07</td>\n",
       "      <td>252664.93</td>\n",
       "      <td>134307.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>119943.24</td>\n",
       "      <td>156547.42</td>\n",
       "      <td>256512.92</td>\n",
       "      <td>132602.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>114523.61</td>\n",
       "      <td>122616.84</td>\n",
       "      <td>261776.23</td>\n",
       "      <td>129917.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78013.11</td>\n",
       "      <td>121597.55</td>\n",
       "      <td>264346.06</td>\n",
       "      <td>126992.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>94657.16</td>\n",
       "      <td>145077.58</td>\n",
       "      <td>282574.31</td>\n",
       "      <td>125370.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>91749.16</td>\n",
       "      <td>114175.79</td>\n",
       "      <td>294919.57</td>\n",
       "      <td>124266.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>86419.70</td>\n",
       "      <td>153514.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>122776.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>76253.86</td>\n",
       "      <td>113867.30</td>\n",
       "      <td>298664.47</td>\n",
       "      <td>118474.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>78389.47</td>\n",
       "      <td>153773.43</td>\n",
       "      <td>299737.29</td>\n",
       "      <td>111313.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>73994.56</td>\n",
       "      <td>122782.75</td>\n",
       "      <td>303319.26</td>\n",
       "      <td>110352.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>67532.53</td>\n",
       "      <td>105751.03</td>\n",
       "      <td>304768.73</td>\n",
       "      <td>108733.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>77044.01</td>\n",
       "      <td>99281.34</td>\n",
       "      <td>140574.81</td>\n",
       "      <td>108552.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>64664.71</td>\n",
       "      <td>139553.16</td>\n",
       "      <td>137962.62</td>\n",
       "      <td>107404.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>75328.87</td>\n",
       "      <td>144135.98</td>\n",
       "      <td>134050.07</td>\n",
       "      <td>105733.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>72107.60</td>\n",
       "      <td>127864.55</td>\n",
       "      <td>353183.81</td>\n",
       "      <td>105008.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>66051.52</td>\n",
       "      <td>182645.56</td>\n",
       "      <td>118148.20</td>\n",
       "      <td>103282.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>65605.48</td>\n",
       "      <td>153032.06</td>\n",
       "      <td>107138.38</td>\n",
       "      <td>101004.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>61994.48</td>\n",
       "      <td>115641.28</td>\n",
       "      <td>91131.24</td>\n",
       "      <td>99937.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>61136.38</td>\n",
       "      <td>152701.92</td>\n",
       "      <td>88218.23</td>\n",
       "      <td>97483.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>63408.86</td>\n",
       "      <td>129219.61</td>\n",
       "      <td>46085.25</td>\n",
       "      <td>97427.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>55493.95</td>\n",
       "      <td>103057.49</td>\n",
       "      <td>214634.81</td>\n",
       "      <td>96778.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>46426.07</td>\n",
       "      <td>157693.92</td>\n",
       "      <td>210797.67</td>\n",
       "      <td>96712.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>46014.02</td>\n",
       "      <td>85047.44</td>\n",
       "      <td>205517.64</td>\n",
       "      <td>96479.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>28663.76</td>\n",
       "      <td>127056.21</td>\n",
       "      <td>201126.82</td>\n",
       "      <td>90708.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>44069.95</td>\n",
       "      <td>51283.14</td>\n",
       "      <td>197029.42</td>\n",
       "      <td>89949.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20229.59</td>\n",
       "      <td>65947.93</td>\n",
       "      <td>185265.10</td>\n",
       "      <td>81229.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38558.51</td>\n",
       "      <td>82982.09</td>\n",
       "      <td>174999.30</td>\n",
       "      <td>81005.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>28754.33</td>\n",
       "      <td>118546.05</td>\n",
       "      <td>172795.67</td>\n",
       "      <td>78239.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>27892.92</td>\n",
       "      <td>84710.77</td>\n",
       "      <td>164470.71</td>\n",
       "      <td>77798.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>23640.93</td>\n",
       "      <td>96189.63</td>\n",
       "      <td>148001.11</td>\n",
       "      <td>71498.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>15505.73</td>\n",
       "      <td>127382.30</td>\n",
       "      <td>35534.17</td>\n",
       "      <td>69758.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>22177.74</td>\n",
       "      <td>154806.14</td>\n",
       "      <td>28334.72</td>\n",
       "      <td>65200.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1000.23</td>\n",
       "      <td>124153.04</td>\n",
       "      <td>1903.93</td>\n",
       "      <td>64926.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1315.46</td>\n",
       "      <td>115816.21</td>\n",
       "      <td>297114.46</td>\n",
       "      <td>49490.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.00</td>\n",
       "      <td>135426.92</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42559.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>542.05</td>\n",
       "      <td>51743.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>35673.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.00</td>\n",
       "      <td>116983.80</td>\n",
       "      <td>45173.06</td>\n",
       "      <td>14681.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    R&D Spend  Administration  Marketing Spend     Profit\n",
       "0   165349.20       136897.80        471784.10  192261.83\n",
       "1   162597.70       151377.59        443898.53  191792.06\n",
       "2   153441.51       101145.55        407934.54  191050.39\n",
       "3   144372.41       118671.85        383199.62  182901.99\n",
       "4   142107.34        91391.77        366168.42  166187.94\n",
       "5   131876.90        99814.71        362861.36  156991.12\n",
       "6   134615.46       147198.87        127716.82  156122.51\n",
       "7   130298.13       145530.06        323876.68  155752.60\n",
       "8   120542.52       148718.95        311613.29  152211.77\n",
       "9   123334.88       108679.17        304981.62  149759.96\n",
       "10  101913.08       110594.11        229160.95  146121.95\n",
       "11  100671.96        91790.61        249744.55  144259.40\n",
       "12   93863.75       127320.38        249839.44  141585.52\n",
       "13   91992.39       135495.07        252664.93  134307.35\n",
       "14  119943.24       156547.42        256512.92  132602.65\n",
       "15  114523.61       122616.84        261776.23  129917.04\n",
       "16   78013.11       121597.55        264346.06  126992.93\n",
       "17   94657.16       145077.58        282574.31  125370.37\n",
       "18   91749.16       114175.79        294919.57  124266.90\n",
       "19   86419.70       153514.11             0.00  122776.86\n",
       "20   76253.86       113867.30        298664.47  118474.03\n",
       "21   78389.47       153773.43        299737.29  111313.02\n",
       "22   73994.56       122782.75        303319.26  110352.25\n",
       "23   67532.53       105751.03        304768.73  108733.99\n",
       "24   77044.01        99281.34        140574.81  108552.04\n",
       "25   64664.71       139553.16        137962.62  107404.34\n",
       "26   75328.87       144135.98        134050.07  105733.54\n",
       "27   72107.60       127864.55        353183.81  105008.31\n",
       "28   66051.52       182645.56        118148.20  103282.38\n",
       "29   65605.48       153032.06        107138.38  101004.64\n",
       "30   61994.48       115641.28         91131.24   99937.59\n",
       "31   61136.38       152701.92         88218.23   97483.56\n",
       "32   63408.86       129219.61         46085.25   97427.84\n",
       "33   55493.95       103057.49        214634.81   96778.92\n",
       "34   46426.07       157693.92        210797.67   96712.80\n",
       "35   46014.02        85047.44        205517.64   96479.51\n",
       "36   28663.76       127056.21        201126.82   90708.19\n",
       "37   44069.95        51283.14        197029.42   89949.14\n",
       "38   20229.59        65947.93        185265.10   81229.06\n",
       "39   38558.51        82982.09        174999.30   81005.76\n",
       "40   28754.33       118546.05        172795.67   78239.91\n",
       "41   27892.92        84710.77        164470.71   77798.83\n",
       "42   23640.93        96189.63        148001.11   71498.49\n",
       "43   15505.73       127382.30         35534.17   69758.98\n",
       "44   22177.74       154806.14         28334.72   65200.33\n",
       "45    1000.23       124153.04          1903.93   64926.08\n",
       "46    1315.46       115816.21        297114.46   49490.75\n",
       "47       0.00       135426.92             0.00   42559.73\n",
       "48     542.05        51743.15             0.00   35673.41\n",
       "49       0.00       116983.80         45173.06   14681.40"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generazione del modello \n",
    "## importo i dati dal csv\n",
    "df = pd.read_csv(\"https://frenzy86.s3.eu-west-2.amazonaws.com/python/data/Startup.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[    R&D Spend  Administration  Marketing Spend\n",
       " 46    1315.46       115816.21        297114.46\n",
       " 47       0.00       135426.92             0.00\n",
       " 15  114523.61       122616.84        261776.23\n",
       " 9   123334.88       108679.17        304981.62\n",
       " 16   78013.11       121597.55        264346.06\n",
       " 24   77044.01        99281.34        140574.81\n",
       " 34   46426.07       157693.92        210797.67\n",
       " 31   61136.38       152701.92         88218.23\n",
       " 0   165349.20       136897.80        471784.10\n",
       " 44   22177.74       154806.14         28334.72\n",
       " 27   72107.60       127864.55        353183.81\n",
       " 33   55493.95       103057.49        214634.81\n",
       " 5   131876.90        99814.71        362861.36\n",
       " 29   65605.48       153032.06        107138.38\n",
       " 11  100671.96        91790.61        249744.55\n",
       " 36   28663.76       127056.21        201126.82\n",
       " 1   162597.70       151377.59        443898.53\n",
       " 21   78389.47       153773.43        299737.29\n",
       " 2   153441.51       101145.55        407934.54\n",
       " 43   15505.73       127382.30         35534.17\n",
       " 35   46014.02        85047.44        205517.64\n",
       " 23   67532.53       105751.03        304768.73\n",
       " 40   28754.33       118546.05        172795.67\n",
       " 10  101913.08       110594.11        229160.95\n",
       " 22   73994.56       122782.75        303319.26\n",
       " 18   91749.16       114175.79        294919.57\n",
       " 49       0.00       116983.80         45173.06\n",
       " 20   76253.86       113867.30        298664.47\n",
       " 7   130298.13       145530.06        323876.68\n",
       " 42   23640.93        96189.63        148001.11\n",
       " 14  119943.24       156547.42        256512.92\n",
       " 28   66051.52       182645.56        118148.20\n",
       " 38   20229.59        65947.93        185265.10,\n",
       "     R&D Spend  Administration  Marketing Spend\n",
       " 13   91992.39       135495.07        252664.93\n",
       " 39   38558.51        82982.09        174999.30\n",
       " 30   61994.48       115641.28         91131.24\n",
       " 45    1000.23       124153.04          1903.93\n",
       " 17   94657.16       145077.58        282574.31\n",
       " 48     542.05        51743.15             0.00\n",
       " 26   75328.87       144135.98        134050.07\n",
       " 25   64664.71       139553.16        137962.62\n",
       " 32   63408.86       129219.61         46085.25\n",
       " 19   86419.70       153514.11             0.00\n",
       " 12   93863.75       127320.38        249839.44\n",
       " 4   142107.34        91391.77        366168.42\n",
       " 37   44069.95        51283.14        197029.42\n",
       " 8   120542.52       148718.95        311613.29\n",
       " 3   144372.41       118671.85        383199.62\n",
       " 6   134615.46       147198.87        127716.82\n",
       " 41   27892.92        84710.77        164470.71,\n",
       " 46     49490.75\n",
       " 47     42559.73\n",
       " 15    129917.04\n",
       " 9     149759.96\n",
       " 16    126992.93\n",
       " 24    108552.04\n",
       " 34     96712.80\n",
       " 31     97483.56\n",
       " 0     192261.83\n",
       " 44     65200.33\n",
       " 27    105008.31\n",
       " 33     96778.92\n",
       " 5     156991.12\n",
       " 29    101004.64\n",
       " 11    144259.40\n",
       " 36     90708.19\n",
       " 1     191792.06\n",
       " 21    111313.02\n",
       " 2     191050.39\n",
       " 43     69758.98\n",
       " 35     96479.51\n",
       " 23    108733.99\n",
       " 40     78239.91\n",
       " 10    146121.95\n",
       " 22    110352.25\n",
       " 18    124266.90\n",
       " 49     14681.40\n",
       " 20    118474.03\n",
       " 7     155752.60\n",
       " 42     71498.49\n",
       " 14    132602.65\n",
       " 28    103282.38\n",
       " 38     81229.06\n",
       " Name: Profit, dtype: float64,\n",
       " 13    134307.35\n",
       " 39     81005.76\n",
       " 30     99937.59\n",
       " 45     64926.08\n",
       " 17    125370.37\n",
       " 48     35673.41\n",
       " 26    105733.54\n",
       " 25    107404.34\n",
       " 32     97427.84\n",
       " 19    122776.86\n",
       " 12    141585.52\n",
       " 4     166187.94\n",
       " 37     89949.14\n",
       " 8     152211.77\n",
       " 3     182901.99\n",
       " 6     156122.51\n",
       " 41     77798.83\n",
       " Name: Profit, dtype: float64]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## definisco i test e train set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=[\"Profit\"]), df[\"Profit\"], random_state=42, test_size=0.33)\n",
    "train_test_split(df.drop(columns=[\"Profit\"]), df[\"Profit\"], random_state=42, test_size=0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## genero il modello \n",
    "\n",
    "model = LinearRegression();\n",
    "model.fit(X_train, y_train)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127262.67887963,  83628.42273586,  99410.80060815,  47272.13837566,\n",
       "       129872.05095853,  49300.62401558, 110383.74620084, 101974.58242029,\n",
       "        98943.34934344, 115633.97224252, 128987.14008892, 172389.58497925,\n",
       "        89744.89077441, 151526.98290452, 173744.6215336 , 158288.28142805,\n",
       "        74633.56534586])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Calcolo gli errori\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "# scarto = y_pred - y_test\n",
    "# scarto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5817.222734586708"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MAE \n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57358376.281903416"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### MSE \n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7573.5312953670045"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RMSE\n",
    "\n",
    "mean_squared_error(y_test, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9596896814469176"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m## creazione dei binari dento alla cartella model di streamlit\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m mlem\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39msave(model, \u001b[39m\"\u001b[39m\u001b[39m../streamlit/model/multiple_reg\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m../streamlit/model/multiple_reg_\u001b[39m\u001b[39m\"\u001b[39m, sample_data\u001b[39m=\u001b[39mX_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlem' is not defined"
     ]
    }
   ],
   "source": [
    "## creazione dei binari dento alla cartella model di streamlit\n",
    "\n",
    "mlem.api.save(model, \"../streamlit/model/multiple_reg\", \"../streamlit/model/multiple_reg_\", sample_data=X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Advanced\n",
    "## loggin in degli utenti\n",
    "\n",
    "Per qualsiasi servizio di log-in c'è bisogno di un sistema per tenere traccia degli utenti registrati (e le loro psw), il nostro programma andrà poi a controllare se alla richiesta di accesso vengono presentate delle credenziali valide.\n",
    "\n",
    "Per semplificare le cose creiamo un file json con le credenziali, in realtà ci servirebbe un DB.\n",
    "\n",
    "Per utilizzare il file jason dobbiamo aprire il file e leggerlo, per aprire il file con python si usa [with](https://docs.python.org/3/reference/compound_stmts.html#with)\n",
    "\n",
    "Per rendere più sicuro il sistema si possono prendere accorgimenti come creare l'has della pagina per evitare di tenere le psw in chiaro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multipage\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
